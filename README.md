# Probabilistic Label Spreading



## Description
This repository contains the codebase for probabilistic label spreading (PLS). 

The folder data contains all files needed to process the datasets where supported datasets include EMNIST-digits, CIFAR10, ANIMALS10, TinyImageNet. Further a Two Moons dataset is generated and a dataset with probabilistic labels generated by the industry is used.
Data processing includes dimensionality reduction techniques such as UMAP, PCA and CLIP embedding where the target dimension varies. The efficient net b7 is fine tuned to each dataset in order to generate probabilistic labels i.e. predicted class probabilities. An exception is the TinyImageNet dataset where a Swin Transformer model was fine tuned using https://github.com/ehuynh1106/TinyImageNet-Transformers.

Dimensionality reduction and fine tuning are computationally expensive as they are conducted for all datasets. It may take a few days on a standard GPU.


## üöÄ Installation

To use this project, you must have:

* A **CUDA-capable GPU**
* **NVIDIA CUDA toolkit** installed and properly configured
* A working **Python environment** (Python 3.9+ recommended)
* [NVIDIA AMGX](https://github.com/NVIDIA/AMGX)
* [PyAMGX](https://github.com/shwina/pyamgx)

### 1. Clone the Repository
```bash
git clone https://github.com/yourusername/yourproject.git
cd yourproject
```

### 2. Create and Activate a Python Virtual Environment (Optional but Recommended)
```bash
python -m venv pls
source pls/bin/activate
```

### 3. Install Python Dependencies

Make sure you have `pip` `setuptools` and `wheel` updated:
```
pip install --upgrade pip setuptools wheel
```
### 3.1 Install PyTorch

Install PyTorch compatible with your CUDA version from [https://pytorch.org/get-started/locally](https://pytorch.org/get-started/locally). For example:

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

> ‚ö†Ô∏è Replace `cu118` with your actual CUDA version (e.g., `cu117`, `cu121`, etc.).

### 3.2 Install the requirements.txt:

```bash
pip install -r requirements.txt
```

### 4. Install NVIDIA AMGX

You need to build and install [NVIDIA AMGX](https://github.com/NVIDIA/AMGX). Follow the official instructions.


### 5. Install PyAMGX

Install `pyamgx` (must be done after AMGX is built):

Follow the official [PyAMGX](https://pyamgx.readthedocs.io/en/latest/install.html) instruction.



### Troubleshooting
#### Installation Problems
- if problems like `ModuleNotFoundError: No module named 'numpy' ` appear durring the PyAmgx installation try to downgrade numpy to numpy==1.26.4 (last version before 2.0.0) and if not done already update pip, setuptools and wheel

- Also before installing `PyAMGX` dont forget to set the correct environment variable for `AMGX_DIR` and `AMGX_BUILD_DIR`

####  Execution Problems
- if problems like `pyamgx.AMGXError: Error initializing amgx core` occur when running this project, check if all environment variables for CUDA and AMGX are correct:
```bash
echo $AMGX_DIR
echo $AMGX_BUILD_DIR
echo $LD_LIBRARY_PATH
echo $CPATH
echo $PATH
```
and check if the `FGMRES.json` file under the `configs` folder is found correctly.


---

You can now run the project. Make sure your CUDA drivers are correctly installed and compatible with your hardware and installed libraries.

---

### Usage

1. As a first step, datasets need to be stored locally under data/datasets and named accordingly (CIFAR10, CIFAR10-H, EMNIST, ANIMALS10, TwoMoons, TinyImageNet)
Then, in the data folder, the script compute_embeddings can be used to apply data pre-processing. Note that this may take up to a day if all target dimensions are to be evaluated. It is sufficient to specify only the single target dimension of 20 here to replicate the main results. 

Please specify the target dimensions for each dataset in the jupyter notebook dimensionality_reduction.ipynb and then run process_data.sh.
This script will also generate the soft labels by executing the notebook prob_label_generation.ipynb. The Two Moons dataset has its own jupyter notebook for data-processing, which is also executed when calling process_data.sh.

2. With the datasets processed our method can be evaluated using the jupyter notebook probabilistic_label_spreading.ipynb

3. Clip zero shot classification and hyperparameter optimization have their own scripts that should also be run for the full performance comparison.

4. Further, experiments are configured in experiments folder and individual experiments can be run with the run_experiment.py {experiment_name} command. Use run_all_experiments.sh to conduct all experiments.

5. Main Results can be analyzed directly with the jupyter notebook analysis.ipynb. Supplementary studies are conducted in runtime.ipynb, performance_est_with_test_data.ipynb and probabilistic_label_spreading.ipynb, which can also be used for debugging if the method should not work.

